# -*- coding: utf-8 -*-
"""585_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3qEcH8Va_8v6xHmqD0kQKbz7dIlFkFi
"""

# Mounts data with stem as 'drive'
from google.colab import drive
drive.mount('/content/drive')

# had to install a few packages, left this here just in case
!pip install PyPDF2
!pip install textract
!pip install nltk

# Imports
import copy
import os
from os import listdir
from os.path import isfile, join
import PyPDF2 
import textract
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
from collections import *
nltk.download('punkt')
nltk.download('stopwords')
import numpy as np

# This line refreshes the known data in the resume folder
!ls "/content/drive/Shared drives/585_final_project/HackHer413_Resumes/"

mypath = "/content/drive/Shared drives/585_final_project/HackHer413_Resumes/"
onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]

#import re
#write a for-loop to open many files -- leave a comment if you'd #like to learn how


texts = []
for filename in onlyfiles:
  f = filename
  filename = mypath+filename
  print(filename)
  #open allows you to read the file
  pdfFileObj = open(filename,'rb')
  #The pdfReader variable is a readable object that will be parsed
  pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict=False)
  #discerning the number of pages will allow us to parse through all #the pages
  num_pages = pdfReader.numPages
  count = 0
  text = ""
  #The while loop will read each page
  while count < num_pages:
      pageObj = pdfReader.getPage(count)
      count +=1
      text += pageObj.extractText()
      text = text.replace('\r','!')
      text = text.replace('\n','')
      text = text.replace('\t','^')
      text = text.replace('\v','*')
      text = text.lower()
  # split into words by white space
  # split into words by white space

  # remove punctuation from each word
  import re
  words = re.split(r'\W+', text)
  #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.
  if text != "":
     text = text
  #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text
  else:
    try:
      text = textract.process(fileurl, method='tesseract', language='eng')
    except:
      text = ""
  # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\n' etc.
  # Now, we will clean our text variable, and return it as a list of keywords.
#   print(text)

  # texts.append(text)
  #words = re.split(r'\W+', text)
  texts.append((f, text))

print('Size of raw dataset:', len(texts))

# '''
# TESTING BLOCK CHANGE WITH CARE
# '''

# filen = "/content/drive/Shared drives/585_final_project/HackHer413_Resumes/20_Leila_Eshghi.pdf"
# pdfFileOb = open(filen,'rb')
# pdfR = PyPDF2.PdfFileReader(pdfFileOb, strict=False)
# # print(pdfR.getPage(0).extractText())
# #The word_tokenize() function will break our text phrases into #individual words
# tokens = word_tokenize(pdfR.getPage(0).extractText())
# #we'll create a new list which contains punctuation we wish to clean
# punctuations = ['(',')',';',':','[',']',',']
# #We initialize the stopwords variable which is a list of words like #"The", "I", "and", etc. that don't hold much value as keywords
# stop_words = stopwords.words('english')
# #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.
# keywords = [word for word in tokens if not word in stop_words and not word in punctuations]
# lk = len(keywords)
# keewords = copy.copy(keywords)
# print(keewords)
# education = ['Education', 'education', 'EDUCATION']
# experience = ['Experience', 'experience', 'EXPERIENCE']
# skills = ['Skills', 'skills', 'SKILLS']
# technical = ['Technical', 'technical', 'TECHNICAL']
# research = ['Research', 'research', 'RESEARCH']
# projects = ['Projects', 'projects', 'PROJECTS']
# objective = ['Objective', 'objective', 'OBJECTIVE']
# activities = ['Activities', 'activities', 'ACTIVITIES']
# for word in range(lk):
#   if (keywords[word] in education or keywords[word] in experience or
#       keywords[word] in skills or keywords[word] in technical or
#       keywords[word] in research or keywords[word] in projects or
#       keywords[word] in objective or keywords[word] in activities):
#     break
#   else:
#     keewords = keewords[1:]
# keewords

'''
anonymize:

DESCRIPTION:
takes in tokenized resume and removes identifying information. Approaches task 
by removing all text before a few 'action' words. This process
also conveniently cleans the data of a few garbage tokens.

PARAMS:
keywords - tokenized data from a single resume

RETURN:
a copy of keywords with id info scrubbed

'''
def anonymize(keywords):
  lk = len(keywords)
  keewords = copy.copy(keywords)
  education = ['Education', 'education', 'EDUCATION']
  school = ['School', 'school', 'SCHOOL']
  experience = ['Experience', 'experience', 'EXPERIENCE']
  skills = ['Skills', 'skills', 'SKILLS']
  technical = ['Technical', 'technical', 'TECHNICAL']
  research = ['Research', 'research', 'RESEARCH']
  projects = ['Projects', 'projects', 'PROJECTS']
  objective = ['Objective', 'objective', 'OBJECTIVE']
  activities = ['Activities', 'activities', 'ACTIVITIES']
  interests = ['Interests', 'interests', 'INTERESTS']
  for word in range(lk):
    if (keywords[word] in education or keywords[word] in experience or
        keywords[word] in skills or keywords[word] in technical or
       keywords[word] in research or keywords[word] in projects or
       keywords[word] in objective or keywords[word] in activities or
       keywords[word] in interests):
      break
    else:
      keewords = keewords[1:]
  return keewords

def make_false(flag_array, target):
  flag_arr = copy.copy(flag_array)
  for flag in flag_arr:
    if flag is not target:
      flag[0] = False
  return flag_arr

'''
categorize:

DESCRIPTION:
Sorts anonymized data into general resume categories retaining order

PARAMS:
keywords - anonymized list of resume data in order-ish

RETURN:
a dictionary of the categorized resume
'''
def categorize(keywords):
  education = ['Education', 'education', 'EDUCATION', 'School', 'school', 'SCHOOL']
  
  # Flag to determine both if we run into the word and are in the 
  # section (flag[0]), as well as if we have seen it before (flag[1])
  # Given nature of reumes, first time we encounter these words is 
  # overwhelmingly the section header
  edu = [False, False]
  experience = ['Experience', 'experience', 'EXPERIENCE']
  exp = [False, False]
  skills = ['Skills', 'skills', 'SKILLS', 'Technical', 'technical', 'TECHNICAL']
  tech = [False, False]
  research = ['Research', 'research', 'RESEARCH']
  res = [False, False]
  projects = ['Projects', 'projects', 'PROJECTS']
  pro = [False, False]
  objective = ['Objective', 'objective', 'OBJECTIVE']
  obj = [False, False]
  activities = ['Activities', 'activities', 'ACTIVITIES']
  act = [False, False]
  interests = ['Interests', 'interests', 'INTERESTS']
  inter = [False, False]
  flags = [edu, exp, tech, res, pro, obj, act, inter]
  categories_without_skills_and_tech = ['Education', 'education', 'EDUCATION',
                                        'School', 'school', 'SCHOOL'
                                       'Experience', 'experience', 'EXPERIENCE',
                                       'Research', 'research', 'RESEARCH',
                                        'Projects', 'projects', 'PROJECTS',
                                        'Objective', 'objective', 'OBJECTIVE',
                                        'Activities', 'activities', 'ACTIVITIES',
                                        'Interests', 'interests', 'INTERESTS']
  all_cats = ['Education', 'education', 'EDUCATION',
              'School', 'school', 'SCHOOL'
              'Experience', 'experience', 'EXPERIENCE',
              'Skills', 'skills', 'SKILLS',
              'Technical', 'technical', 'TECHNICAL',
              'Research', 'research', 'RESEARCH',
              'Projects', 'projects', 'PROJECTS',
              'Objective', 'objective', 'OBJECTIVE',
              'Activities', 'activities', 'ACTIVITIES',
              'Interests', 'interests', 'INTERESTS']
  
  categories = {'education':[], 'experience':[], 'skills':[], 'research':[],
                'projects':[], 'objective':[], 'activities':[], 'interests':[]}
  words = copy.copy(keywords)
  '''
  this counter + counter_val are to prevent accidentally going into the next
  section
  ** in future be sure to check for 'research intern' or 'research assistant' **
  '''
  counter = 0
  count_val = 3
  for word in words:
    if (word in education and edu[1] == False and counter <= 0):
      edu[0] = True
      edu[1] = True
      counter = count_val
      flags = make_false(flags, edu)
    elif (word in experience and exp[1] == False and counter <= 0):
      exp[0] = True
      exp[1] = True
      counter = count_val
      flags = make_false(flags, exp)
    elif (word in skills and tech[1] == False and counter <= 0):
      tech[0] = True
      tech[1] = True
      counter = count_val
      flags = make_false(flags, tech)
    elif (word in research and res[1] == False and counter <= 0):
      res[0] = True
      res[1] = True
      counter = count_val
      flags = make_false(flags, res)
    elif (word in projects and pro[1] == False and counter <= 0):
      pro[0] = True
      pro[1] = True
      counter = count_val
      flags = make_false(flags, pro)
    elif (word in objective and obj[1] == False and counter <= 0):
      obj[0] = True
      obj[1] = True
      counter = count_val
      flags = make_false(flags, obj)
    elif (word in activities and act[1] == False and counter <= 0):
      act[0] = True
      act[1] = True
      counter = count_val
      flags = make_false(flags, act)
    elif (word in interests and inter[1] == False and counter <= 0):
      inter[0] = True
      inter[1] = True
      counter = count_val
      flags = make_false(flags, inter)
    
    if (edu[0] and word not in education):
      categories['education'].append(word)
      counter -=1
    if (exp[0] and word not in experience):
      categories['experience'].append(word)
      counter -=1
    if (tech[0] and word not in skills):
      categories['skills'].append(word)
      counter -=1
    if (res[0] and word not in research):
      categories['research'].append(word)
      counter -=1
    if (pro[0] and word not in projects):
      categories['projects'].append(word)
      counter -=1
    if (obj[0] and word not in objective):
      categories['objective'].append(word)
      counter -=1
    if (act[0] and word not in activities):
      categories['activities'].append(word)
      counter -=1
    if (inter[0] and word not in interests):
      categories['interests'].append(word)
      counter -=1
  return categories

tokenized_keywords = []
tokenized_categories = []
success_files = []
c = 0
for t in texts:
  #The word_tokenize() function will break our text phrases into #individual words
  tokens = word_tokenize(t[1])
  #we'll create a new list which contains punctuation we wish to clean
  punctuations = ['(',')',';',':','[',']',',']
  #We initialize the stopwords variable which is a list of words like #"The", "I", "and", etc. that don't hold much value as keywords
  stop_words = stopwords.words('english')
  #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.
  keywords = [word for word in tokens if not word in stop_words and not word in punctuations]
  if keywords != []:
    k = anonymize(keywords)
    cats = categorize(k)
    if k == []:
      c += 1
    else:
      tokenized_keywords.append((t[0], k))
      tokenized_categories.append((t[0],cats))
      success_files.append(t[0])

print('Size of cleaned dataset:',len(tokenized_keywords))
print(tokenized_categories[0:2])

# Small test to see how we're doing
# Should not see any names or identifying informations
# print(success_files)
# print(stop_words)
tokenized_categories[0]
# success_file = "/content/drive/Shared drives/585_final_project/success_file.csv"
# with open(success_file, 'w') as myfile:
#   for entry in success_files:
#     myfile.write(entry)

def loadGloveModel(gloveFile):
    print("Loading Glove Model")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print("Done.",len(model)," words loaded!")
    return model

glove_file = "/content/drive/Shared drives/585_final_project/glove_words.txt"
glove_model = loadGloveModel(glove_file)

print('the' in glove_model)

model_embeddings = {}
for tokenized_resume in tokenized_categories:
  resume_embeddings = {}
  # print(tokenized_resume)
  for cat in tokenized_resume[1]:
    # print(len(tokenized_resume[1][cat]))
    category_embeddings = {}
    for word in tokenized_resume[1][cat]:
      # print(word)
      if word in glove_model:
        if word in category_embeddings:
          # print(word)
          index = category_embeddings[word][0]
          index += 1
          category_embeddings[word] = (index, glove_model[word])
        else:
          category_embeddings[word] = (1, glove_model[word])
    # print(len(category_embeddings))
    if cat in resume_embeddings:
      index = resume_embeddings[cat][0]
      index += 1
      resume_embeddings[cat] = (index, category_embeddings)
    else:
      # print(category_embeddings)
      resume_embeddings[cat] = (1, category_embeddings)
  if tokenized_resume[0] in model_embeddings:
    index = model_embeddings[tokenized_resume[0]][0]
    index += 1
    model_embeddings[tokenized_resume[0]] = (index, resume_embeddings)
  else:
    model_embeddings[tokenized_resume[0]] = (1, resume_embeddings)

# Sanity check to make sure it creates the embeddings as expected
model_embeddings['100_Tanisha_Nalavadi.pdf']

"""# Sources


*   To parse the PDF: https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f
*   Setting up environment: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q#scrollTo=H4SJ-tGNkOeY
"""